{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import monai\n",
    "import dataloader\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim.lr_scheduler\n",
    "from monai.losses import DiceLoss\n",
    "from monai.losses import FocalLoss\n",
    "from monai.networks.nets import UNet\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import BinaryPrecision, BinaryRecall\n",
    "import math\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/hm30rad/'\n",
    "df = pd.read_csv('../data/train_labels.csv')\n",
    "names = df['tomo_id'].astype(str).unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-31 16:38:50,902] A new study created in memory with name: no-name-4b75cb66-9ee1-415c-9a92-289e6af693d8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "LR: 5.000e-04 Gamma: 1.0000 | Pos Weight: 1.0000\n",
      "Batch Loss: 0.3614 Pred Avg: -0.1451 | Tgt Avg: 0.0009\n",
      "Pred std / var: 0.1569,0.0246, Tgt std / var: 0.0285,0.0008\n",
      "Validation Precision: 0.0020, Recall: 0.0028, F2.0-score: 0.0026\n",
      "\n",
      "Epoch 1\n",
      "LR: 5.000e-04 Gamma: 1.0500 | Pos Weight: 1.1000\n",
      "Batch Loss: 0.1411 Pred Avg: -0.0669 | Tgt Avg: 0.0016\n",
      "Pred std / var: 0.1069,0.0114, Tgt std / var: 0.0084,0.0001\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 2\n",
      "LR: 5.000e-04 Gamma: 1.1025 | Pos Weight: 1.2100\n",
      "Batch Loss: 0.0727 Pred Avg: -0.0259 | Tgt Avg: 0.0017\n",
      "Pred std / var: 0.0800,0.0064, Tgt std / var: 0.0161,0.0003\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 3\n",
      "LR: 5.000e-04 Gamma: 1.1576 | Pos Weight: 1.3310\n",
      "Batch Loss: 0.0390 Pred Avg: 0.0013 | Tgt Avg: 0.0010\n",
      "Pred std / var: 0.0680,0.0046, Tgt std / var: 0.0018,0.0000\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 4\n",
      "LR: 4.500e-04 Gamma: 1.2155 | Pos Weight: 1.4641\n",
      "Batch Loss: 0.0305 Pred Avg: 0.0019 | Tgt Avg: 0.0006\n",
      "Pred std / var: 0.0593,0.0035, Tgt std / var: 0.0185,0.0003\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 5\n",
      "LR: 4.500e-04 Gamma: 1.2763 | Pos Weight: 1.6105\n",
      "Batch Loss: 0.0252 Pred Avg: 0.0011 | Tgt Avg: 0.0010\n",
      "Pred std / var: 0.0532,0.0028, Tgt std / var: 0.0280,0.0008\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 6\n",
      "LR: 4.500e-04 Gamma: 1.3401 | Pos Weight: 1.7716\n",
      "Batch Loss: 0.0209 Pred Avg: 0.0000 | Tgt Avg: 0.0005\n",
      "Pred std / var: 0.0484,0.0023, Tgt std / var: 0.0063,0.0000\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 7\n",
      "LR: 4.500e-04 Gamma: 1.4071 | Pos Weight: 1.9487\n",
      "Batch Loss: 0.0185 Pred Avg: 0.0000 | Tgt Avg: 0.0009\n",
      "Pred std / var: 0.0447,0.0020, Tgt std / var: 0.0118,0.0001\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 8\n",
      "LR: 4.050e-04 Gamma: 1.4775 | Pos Weight: 2.1436\n",
      "Batch Loss: 0.0157 Pred Avg: 0.0000 | Tgt Avg: 0.0006\n",
      "Pred std / var: 0.0425,0.0018, Tgt std / var: 0.0188,0.0004\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 9\n",
      "LR: 4.050e-04 Gamma: 1.5513 | Pos Weight: 2.3579\n",
      "Batch Loss: 0.0152 Pred Avg: 0.0003 | Tgt Avg: 0.0014\n",
      "Pred std / var: 0.0377,0.0014, Tgt std / var: 0.0098,0.0001\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 10\n",
      "LR: 4.050e-04 Gamma: 1.5000 | Pos Weight: 100.0000\n",
      "Batch Loss: 0.0136 Pred Avg: 0.0003 | Tgt Avg: 0.0010\n",
      "Pred std / var: 0.0369,0.0014, Tgt std / var: 0.0323,0.0010\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 11\n",
      "LR: 4.050e-04 Gamma: 1.5750 | Pos Weight: 110.0000\n",
      "Batch Loss: 0.0122 Pred Avg: 0.0004 | Tgt Avg: 0.0009\n",
      "Pred std / var: 0.0348,0.0012, Tgt std / var: 0.0265,0.0007\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 12\n",
      "LR: 3.645e-04 Gamma: 1.6538 | Pos Weight: 121.0000\n",
      "Batch Loss: 0.0108 Pred Avg: 0.0001 | Tgt Avg: 0.0003\n",
      "Pred std / var: 0.0325,0.0011, Tgt std / var: 0.0147,0.0002\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 13\n",
      "LR: 3.645e-04 Gamma: 1.7364 | Pos Weight: 133.1000\n",
      "Batch Loss: 0.0105 Pred Avg: 0.0002 | Tgt Avg: 0.0014\n",
      "Pred std / var: 0.0302,0.0009, Tgt std / var: 0.0128,0.0002\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 14\n",
      "LR: 3.645e-04 Gamma: 1.8233 | Pos Weight: 146.4100\n",
      "Batch Loss: 0.0095 Pred Avg: 0.0003 | Tgt Avg: 0.0005\n",
      "Pred std / var: 0.0297,0.0009, Tgt std / var: 0.0184,0.0003\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 15\n",
      "LR: 3.645e-04 Gamma: 1.9144 | Pos Weight: 161.0510\n",
      "Batch Loss: 0.0098 Pred Avg: 0.0007 | Tgt Avg: 0.0024\n",
      "Pred std / var: 0.0271,0.0007, Tgt std / var: 0.0199,0.0004\n",
      "Validation Precision: 0.0000, Recall: 0.0000, F2.0-score: 0.0000\n",
      "\n",
      "Epoch 16\n",
      "LR: 3.281e-04 Gamma: 2.0101 | Pos Weight: 177.1561\n",
      "Batch Loss: 0.0085 Pred Avg: 0.0001 | Tgt Avg: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-31 16:53:34,257] Trial 0 failed with parameters: {'zero_threshold': 0.6715573237529597} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hmhor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\hmhor\\AppData\\Local\\Temp\\ipykernel_20364\\2803042850.py\", line 161, in objective\n",
      "    for batch in val_loader:\n",
      "  File \"c:\\Users\\hmhor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"c:\\Users\\hmhor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1317, in _next_data\n",
      "    self._shutdown_workers()\n",
      "  File \"c:\\Users\\hmhor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"c:\\Users\\hmhor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"c:\\Users\\hmhor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\popen_spawn_win32.py\", line 108, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-31 16:53:34,257] Trial 0 failed with value None.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from monai.losses import DiceLoss\n",
    "from monai.networks.nets import UNet\n",
    "from optuna.pruners import MedianPruner\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # ------------------------------ #\n",
    "    #        HYPERPARAMETERS         #\n",
    "    # ------------------------------ #\n",
    "    # lr = trial.suggest_float(\"lr\", 5e-6, 5e-5, log=True)\n",
    "    lr = 5e-4\n",
    "    zero_threshold = trial.suggest_float(\"zero_threshold\", 0.4, 0.75)\n",
    "    \n",
    "    # theta = trial.suggest_float(\"theta\", 0.1, 0.9)\n",
    "    # warmup_epochs = trial.suggest_int(\"warmup_epochs\", 8, 12)\n",
    "    warmup_epochs=10\n",
    "    pos_weight_val = 100\n",
    "    gamma = 1.5\n",
    "    decay = 0.9\n",
    "    # reg_str=1e-3\n",
    "    dropout=0.3\n",
    "    num_epochs = 35\n",
    "    batch_size = 32\n",
    "    # pos_weight = torch.tensor([pos_weight_val], device=device)  # Replace 'value' with your desired pos_weight\n",
    "    \n",
    "    ''' Unused Hyperparameters\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.25, 0.5)\n",
    "    pos_weight_val = trial.suggest_float(\"pos_weight\", 1, 1e4, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.75, 4.0)\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.25, 1.0)\n",
    "    decay = trial.suggest_float('decay', 0.6, 1.0)\n",
    "    reg_str = trial.suggest_float(\"regularization_strength\", 1e-4, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # ------------------------------ #\n",
    "    #              DATA              #\n",
    "    # ------------------------------ #\n",
    "    aug_params = {\n",
    "        \"patch_size\": (108,108,108),\n",
    "        \"final_size\":   (108,108,108),\n",
    "        \"flip_prob\":  0.5,\n",
    "        \"rot_prob\":   0.5,\n",
    "        \"scale_prob\": 1.0,\n",
    "        \"rot_range\":  np.pi,\n",
    "        \"scale_range\": 0.2\n",
    "    }\n",
    "    \n",
    "    train_names, val_names = train_test_split(names, test_size=0.2)\n",
    "\n",
    "    train_dataset = dataloader.MMapDataset(names=train_names, path=data_path, gpu=True, aug_params=aug_params, zero_threshold=zero_threshold)\n",
    "    val_dataset = dataloader.MMapDataset(names=val_names, path=data_path, gpu=True, aug_params=aug_params)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=6, collate_fn=dataloader.custom_collate, shuffle=True, pin_memory=True) # can put data aug in collate func later to have optional aug\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=6, collate_fn=dataloader.custom_collate, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # ------------------------------ #\n",
    "    #             MODEL              #\n",
    "    # ------------------------------ #\n",
    "    model = UNet(\n",
    "        spatial_dims=3,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        channels=(64, 128, 256),\n",
    "        strides=(3, 3),\n",
    "        num_res_units=2,\n",
    "        dropout=dropout,\n",
    "        act = 'relu',\n",
    "        bias=False\n",
    "    ).to(device)\n",
    "    \n",
    "    # ------------------------------ #\n",
    "    #        TRAINING METHODS        #\n",
    "    # ------------------------------ #\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    lr_scheduler = StepLR(optimizer, step_size=4, gamma=decay)\n",
    "    pos_weight_scheduler = dataloader.Scheduler(warmup_epochs=warmup_epochs, warmup_value=1.0, init_value=pos_weight_val, factor=1.1, stop=20)\n",
    "    gamma_scheduler = dataloader.Scheduler(warmup_epochs=warmup_epochs, warmup_value=1.0, init_value=gamma, factor=1.05, stop=10)\n",
    "\n",
    "    # dice_loss = DiceLoss()\n",
    "    def add_regularization_loss(model, regularization_strength):\n",
    "        reg_loss = 0\n",
    "        for param in model.parameters():\n",
    "            reg_loss += torch.sum(param ** 2)\n",
    "        return regularization_strength * reg_loss\n",
    "    \n",
    "    ''' Unused Loss Functions\n",
    "    # dice_loss = DiceLoss(to_onehot_y=False, softmax=True, weight=weights).to(device)\n",
    "    # focal_loss = FocalLoss(to_onehot_y=False, use_softmax=True, weight=weights, gamma=gamma ).to(device)\n",
    "    # mse_loss = nn.MSELoss(reduction='none')\n",
    "    '''\n",
    "    \n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=decay)\n",
    "    i=0\n",
    "    for epoch in range(num_epochs):\n",
    "    # ------------------------------ #\n",
    "    #             TRAIN              #\n",
    "    # ------------------------------ #\n",
    "        gamma = gamma_scheduler()\n",
    "        pos_weight = pos_weight_scheduler()\n",
    "        \n",
    "        focal_loss = FocalLoss(gamma=gamma)\n",
    "        bce_loss = BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n",
    "    \n",
    "        model.train()\n",
    "        batch_loss = 0\n",
    "        opt_avg = 0\n",
    "        tgt_avg = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input, target = batch['src'].to(device), batch['tgt'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(input)\n",
    "            \n",
    "            # output = torch.sigmoid(output)\n",
    "            opt_avg += torch.mean(output)\n",
    "            tgt_avg += torch.mean(target)\n",
    "            # loss = 0.5 * focal_loss(output, target) + 0.5 * bce_loss(output,target)\n",
    "            loss = (output.mean() - target.mean()).abs() + (output.var() - target.var()).abs()\n",
    "            # loss = focal_loss(output,target)\n",
    "            # reg_loss = add_regularization_loss(model, reg_str)\n",
    "            # loss+=reg_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print(f'epoch {epoch} batch {i} loss: {loss.item():.2f}')\n",
    "            batch_loss+=loss.item()\n",
    "        print(f'Epoch {i}')\n",
    "        print(f'LR: {lr_scheduler.get_last_lr()[0]:.3e} Gamma: {gamma_scheduler():.4f} | Pos Weight: {pos_weight_scheduler():.4f}')\n",
    "        print(f'Batch Loss: {(batch_loss / len(train_loader)):.4f} Pred Avg: {(opt_avg / len(train_loader)):.4f} | Tgt Avg: {(tgt_avg / len(train_loader)):.4f}')\n",
    "        # print(f'Total Time: {total_time:.2f}s, Data Load Time: {(total_time - train_forward_time - train_backward_time):.2f}s, Forward Time: {train_forward_time:.2f}s, Backward Time: {train_backward_time:.2f}s')\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        pos_weight_scheduler.step()\n",
    "        gamma_scheduler.step()\n",
    "        \n",
    "        # ---------- #\n",
    "        # VALIDATION #\n",
    "        # ---------- #\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        beta = 2.0  # Adjust for F1-score, for example, or set another value for different balance\n",
    "        precision_metric = BinaryPrecision().to(device)\n",
    "        recall_metric = BinaryRecall().to(device)\n",
    "        sum_x = 0\n",
    "        sum_x2 = 0\n",
    "        count_x =0\n",
    "        sum_y = 0\n",
    "        sum_y2 = 0\n",
    "        count_y =0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input, target = batch['src'].to(device), batch['tgt'].to(device)\n",
    "                output = model(input)\n",
    "                # Threshold predictions to binary (e.g., 0.5)\n",
    "                preds = output  # Apply sigmoid to get probabilities\n",
    "                preds = (preds > 0.6).float()  # Threshold at 0.5\n",
    "\n",
    "                # Threshold the target similarly\n",
    "                target_bin = (target > 0.6).float()  # Convert target to binary based on the same threshold\n",
    "\n",
    "                # Update metrics\n",
    "                precision_metric.update(preds, target_bin)\n",
    "                recall_metric.update(preds, target_bin)\n",
    "\n",
    "                sum_x += output.sum()\n",
    "                sum_x2 += (output ** 2).sum()\n",
    "                count_x += output.numel()\n",
    "                sum_y += target.sum()\n",
    "                sum_y2 += (target ** 2).sum()\n",
    "                count_y += target.numel()\n",
    "\n",
    "            mean_x = sum_x / count_x\n",
    "            var_x = (sum_x2 / count_x) - (mean_x ** 2)\n",
    "            std_x = torch.sqrt(var_x)\n",
    "            \n",
    "            mean_y = sum_y / count_y\n",
    "            var_y = (sum_y2 / count_y) - (mean_y ** 2)\n",
    "            std_y = torch.sqrt(var_y)\n",
    "\n",
    "            # Compute the final metrics\n",
    "            precision = precision_metric.compute()\n",
    "            recall = recall_metric.compute()\n",
    "            fbeta = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n",
    "        if math.isnan(fbeta):\n",
    "            fbeta = 0.0\n",
    "        # Print out precision, recall, and F-beta\n",
    "        print(f\"Pred std / var: {std_x:.4f},{var_x:.4f}, Tgt std / var: {std_y:.4f},{var_y:.4f}\")\n",
    "        print(f\"Validation Precision: {precision:.4f}, Recall: {recall:.4f}, F{beta}-score: {fbeta:.4f}\")\n",
    "        print()\n",
    "\n",
    "        # Report to Optuna or any other hyperparameter optimization tool\n",
    "        # val_loss /= len(val_loader)\n",
    "        trial.report(fbeta, epoch)\n",
    "        # print(f\"Epoch {epoch} loss: {val_loss:.2f}\")\n",
    "            \n",
    "        if trial.should_prune():\n",
    "            \n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        i+=1\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=18))\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
